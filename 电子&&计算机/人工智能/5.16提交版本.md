
#### 1.为什么输入时不是X与Wh点积而是Wh乘上X?
	假设输入层有 n 个神经元，隐藏层有 m 个神经元，那么权重矩阵 W_h 的形状是 m×n，输入向量 X 的形状是 n×1。矩阵乘法 W_h ⋅ X 的结果是一个 m×1 的列向量，表示每个隐藏层神经元的输入。因此，正确的计算方式是 W_h ⋅ X，而不是 X ⋅ W_h。
#### 2.### 输出层误差和隐藏层误差的计算公式如何得到?
	输出层误差和隐藏层误差的计算公式是基于梯度下降法和链式法则推导出来的，用于计算每个神经元对损失函数的贡献。

##### 输出层误差

输出层误差公式为：

δo​=(D−Oout​)⋅Oout​⋅(1−Oout​)

解释：

- D 是目标值。
    
- Oout​ 是输出层的输出值。
    
- Oout​⋅(1−Oout​) 是 Sigmoid 函数的导数，表示输出层输出对输入的敏感度。
    
![[Pasted image 20250516224810.png]]

#### 隐藏层误差

隐藏层误差公式为：

δh​=δo​⋅Wo​T⋅Hout​⋅(1−Hout​)

解释：

- δo​⋅Wo​T 将输出层的误差通过权重矩阵的转置传播回隐藏层。
    
- Hout​⋅(1−Hout​) 是隐藏层输出对输入的敏感度，同样由 Sigmoid 函数的导数给出。
#### **1. 前向传播计算各神经元的输出**

**隐藏层输入：**
$$
\mathbf{H} = \mathbf{X} \cdot \mathbf{W_h} + \mathbf{B_h}
$$
计算每个隐藏层神经元的输入：
- $$ H_1 = 1 \times 1.0 + 2 \times 0.3 + 0.1 = 1.0 + 0.6 + 0.1 = 1.7 $$
- $$ H_2 = 1 \times 0.2 + 2 \times 0.4 + (-0.2) = 0.2 + 0.8 - 0.2 = 0.8 $$
- $$ H_3 = 1 \times 0.5 + 2 \times 0.3 + 0.3 = 0.5 + 0.6 + 0.3 = 1.4 $$

**隐藏层输出（应用 Sigmoid 激活函数）：**
$$
\mathbf{H}_{\text{out}} = \text{Sigmoid}(\mathbf{H})
$$
计算每个隐藏层神经元的输出：
- $$ H_{\text{out}_1} = \frac{1}{1 + e^{-1.7}} \approx 0.8455 $$
- $$ H_{\text{out}_2} = \frac{1}{1 + e^{-0.8}} \approx 0.7090 $$
- $$ H_{\text{out}_3} = \frac{1}{1 + e^{-1.4}} \approx 0.8029 $$

**输出层输入：**
$$
O = \mathbf{H}_{\text{out}} \cdot \mathbf{W_o} + \mathbf{B_o}
$$
计算输出层输入：
$$
O = 0.8455 \times 0.2 + 0.7090 \times 0.8 + 0.8029 \times 1.0 + 0.1 = 0.1691 + 0.5672 + 0.8029 + 0.1 = 1.640
$$

**输出层输出（应用 Sigmoid 激活函数）：**
$$
O_{\text{out}} = \frac{1}{1 + e^{-1.640}} \approx 0.8353
$$

#### **2. 反向传播计算各神经元的局部梯度**

**输出层误差：**
$$
\delta_o = (D - O_{\text{out}}) \cdot O_{\text{out}} \cdot (1 - O_{\text{out}})
$$
计算输出层误差：
$$
\delta_o = (1.0 - 0.8353) \cdot 0.8353 \cdot (1 - 0.8353) \approx 0.1647 \cdot 0.8353 \cdot 0.1647 \approx 0.0224
$$

**隐藏层误差：**
$$
\delta_h = \delta_o \cdot \mathbf{W_o}^T \cdot \mathbf{H}_{\text{out}} \cdot (1 - \mathbf{H}_{\text{out}})
$$
计算每个隐藏层神经元的误差：
- $$ \delta_{h1} = 0.0224 \times 0.2 \times 0.8455 \times (1 - 0.8455) \approx 0.0224 \times 0.2 \times 0.8455 \times 0.1545 \approx 0.00055 $$
- $$ \delta_{h2} = 0.0224 \times 0.8 \times 0.7090 \times (1 - 0.7090) \approx 0.0224 \times 0.8 \times 0.7090 \times 0.2910 \approx 0.0036 $$
- $$ \delta_{h3} = 0.0224 \times 1.0 \times 0.8029 \times (1 - 0.8029) \approx 0.0224 \times 1.0 \times 0.8029 \times 0.1971 \approx 0.0034 $$

#### **3. 利用梯度下降法更新权重和偏置**

**更新隐藏层到输出层的权重：**
$$
\mathbf{W_o} = \mathbf{W_o} + \eta \cdot \delta_o \cdot \mathbf{H}_{\text{out}}^T
$$
计算更新后的权重：
- $$ W_{o1} = 0.2 + 0.1 \times 0.0224 \times 0.8455 \approx 0.2 + 0.00019 \approx 0.20019 $$
- $$ W_{o2} = 0.8 + 0.1 \times 0.0224 \times 0.7090 \approx 0.8 + 0.00159 \approx 0.80159 $$
- $$ W_{o3} = 1.0 + 0.1 \times 0.0224 \times 0.8029 \approx 1.0 + 0.00179 \approx 1.00179 $$

**更新隐藏层偏置：**
$$
\mathbf{B_o} = \mathbf{B_o} + \eta \cdot \delta_o
$$
计算更新后的偏置：
$$
B_o = 0.1 + 0.1 \times 0.0224 \approx 0.1 + 0.00224 \approx 0.10224
$$

**更新输入层到隐藏层的权重：**
$$
\mathbf{W_h} = \mathbf{W_h} + \eta \cdot \delta_h \cdot \mathbf{X}^T
$$
计算更新后的权重：
- $$ W_{h11} = 1.0 + 0.1 \times 0.00055 \times 1 \approx 1.0 + 0.000055 \approx 1.000055 $$
- $$ W_{h12} = 0.2 + 0.1 \times 0.00055 \times 2 \approx 0.2 + 0.00011 \approx 0.20011 $$
- $$ W_{h21} = 0.3 + 0.1 \times 0.0036 \times 1 \approx 0.3 + 0.00036 \approx 0.30036 $$
- $$ W_{h22} = 0.4 + 0.1 \times 0.0036 \times 2 \approx 0.4 + 0.00072 \approx 0.40072 $$
- $$ W_{h31} = 0.5 + 0.1 \times 0.0034 \times 1 \approx 0.5 + 0.00034 \approx 0.50034 $$
- $$ W_{h32} = 0.3 + 0.1 \times 0.0034 \times 2 \approx 0.3 + 0.00068 \approx 0.30068 $$

**更新输入层偏置：**
$$
\mathbf{B_h} = \mathbf{B_h} + \eta \cdot \delta_h
$$
计算更新后的偏置：
- $$ B_{h1} = 0.1 + 0.1 \times 0.00055 \approx 0.1 + 0.000055 \approx 0.100055 $$
- $$ B_{h2} = -0.2 + 0.1 \times 0.0036 \approx -0.2 + 0.00036 \approx -0.19964 $$
- $$ B_{h3} = 0.3 + 0.1 \times 0.0034 \approx 0.3 + 0.00034 \approx 0.30034 $$
